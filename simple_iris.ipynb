{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A SIMPLE INTRODUCTION TO NEURAL NETWORKS WITH PYTHON\n",
    "\n",
    "The goal of this notebook is to provide a simple, straightforward implementation of a multilayer neural network so we can explore various concepts and building blocks you will run into again and again while working with neural nets. Many great examples exist online that are far more robust. However, if you are new to machine learning, the added complexity of these tutorials comes with a cognitive tax - there is too much to learn. I decided to write the canonical python neural network I wish I would have found when I was first starting my journey into deep learning.\n",
    "\n",
    "By implementing a two layer neural network using only Python and Numpy (with a tiny sprinkling of scikit-learn), I hope we can explore each step in consumable manner. Future tutorials will bring in more robust frameworks that we will call upon as our projects get more powerful and creative, such as Google's **Tensorflow** or Microsoft's **Cognitive Toolkit**. These are the two frameworks I use most at work.\n",
    "\n",
    "But being to write a full neural network from basic Python building blocks will be a great first step to understanding what is actually happening under the hood without getting too lost along the way. Let's get started...\n",
    "\n",
    "\n",
    "## Step 1: Loading and preparing our data\n",
    "\n",
    "We will build a neural network that can classify which type of Iris we are analyzing based on certain measurements of the flower. We will be using the famous [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) first presented in 1938 by Fisher and Anderson. \n",
    "\n",
    "There are three types of Irises the dataset classifies among its 150 records. These classes are:\n",
    "\n",
    "![IRISES](images/irises.png)\n",
    "\n",
    "Each of these three labels have 50 records of flower measurments each (for a total of 150 records in the full dataset). Each flower record contains four measurements we will use as our input *features*. These features record the sepal length, sepal width, petal length and petal width for each iris. \n",
    "\n",
    "Fortunately, since the Iris dataset is very famous, it already comes as one of the default datasets that ships with scikit-learn. So let's go ahead and grab it from there:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE WILL BE BUILDING OUR NEURAL NETWORK IN PYTHON, AND MAINLY USE NUMPY FOR OUR OPERATIONS\n",
    "import numpy as np\n",
    "\n",
    "# WHILE SCIKIT-LEARN COMES WITH A LOT OF MACHINE LEARNING HELPER CLASSES, WE WILL LIMIT OURSELVES\n",
    "# TO ONLY USING IT AS AN EASY WAY TO GET US ACCESS TO THE IRIS DATASET\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "features = iris.data\n",
    "labels = iris.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset already seperates the measurement *features* we will be using as our inputs, and the corresponding category *label* for that record that tells us which type of iris we are evaluating.\n",
    "\n",
    "Let's look at the features of the first five records in our dataset to see what the measurements look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total feature records: 150\n",
      "\n",
      "First five records:\n",
      " [[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 4.6  3.1  1.5  0.2]\n",
      " [ 5.   3.6  1.4  0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"total feature records:\", len(features))\n",
    "print(\"\\nFirst five records:\\n\", features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, each record has four measurements. This means when we build our Neural Network, our **input layer** will need four neurons to hold each measurement.\n",
    "\n",
    "Now let's look at the labels for all the records in the dataset. There should be 150 of them, which will match up 1x1 with the 150 feature records we saw above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 150\n",
      "\n",
      "Label records:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"total labels:\", len(labels))\n",
    "print(\"\\nLabel records:\\n\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our simple neural network will be updating itself a record at a time, we may want to shuffle up the order to get a better training distribution for our neural network to learn.\n",
    "\n",
    "Since we already have the features and labels split, however, we need to make sure that we shuffle them both in the same order, otherwise we'll have broken our training data if the labels don't match up to their features anymore!\n",
    "\n",
    "Here's an easy way to do that in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels: 150\n",
      "\n",
      "Label records:\n",
      " [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0 1 2 2 1 2 1 2 1 0 2 1 0 0 0 1 2 0 0 0 1 0 1 2 0 1 2 0 2 2\n",
      " 1 1 2 1 0 1 2 0 0 1 1 0 2 0 0 1 1 2 1 2 2 1 0 0 2 2 0 0 0 1 2 0 2 2 0 1 1\n",
      " 2 1 2 0 2 1 2 1 1 1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1 2 2 0 1 2 0\n",
      " 1 2]\n"
     ]
    }
   ],
   "source": [
    "# LET'S CREATE A VECTOR OF INDICES FOR EACH FEATURE AND THEN SHUFFLE IT\n",
    "idx = np.arange(features.shape[0])\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# AS LONG AS BOTH THE FEATURES AND LABELS USE THE SAME INDEX VECTOR, WE'RE STILL ALL GOOD\n",
    "features = features[idx]\n",
    "labels = labels[idx]\n",
    "\n",
    "# NOW LET'S LOOK AT THE LABELS AGAIN AND MAKE SURE THEY'RE SHUFFLED\n",
    "print(\"total labels:\", len(labels))\n",
    "print(\"\\nLabel records:\\n\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, we have three classes of Irises so each label is either a 0, 1, or 2 (for Setosa, Versicolor, or Verginica respectively).\n",
    "\n",
    "Now, the Iris dataset knows exactly what type of flower each record relates to, so it can give a single label. However, the purpose of a trained, classification neural network is to generate a prediction that takes into account all *possible categories* it has been trained on. That means that for each record, the Neural Network will not generate a single answer, but give each possible label an individual score across a *probability distribution.* \n",
    "\n",
    "Basically, that means that if we have three categories (Setosa, Versicolor, Verginica), each record will get a triple score that looks something like [0.89  .07 .04], where the category with the highest score indicates the highest probability. In this example, that means the prediction would be Setosa, with a probability score of 89%.\n",
    "\n",
    "In order to properly train this network, therefore, we need to reformat all single valued training labels into a probability distribution across the three categories. Since we are 100% sure the labels in our training data are correct, each probability distribution only needs one class scored with a 1 and the other possible class labels zeroed out. \n",
    "\n",
    "It looks like this for our three classes:\n",
    "\n",
    "'0' becomes [1  0  0]\n",
    "\n",
    "'1' becomes [0  1  0]\n",
    "\n",
    "'2' becomes [0  0  1]\n",
    "\n",
    "This is why it is called **one hot encoding** and you can code it like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original first 5 labels\n",
      " [1 0 2 1 1]\n",
      "\n",
      "The first 5 one hot lables\n",
      " [[0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "total_classes = 3\n",
    "\n",
    "def convert_to_one_hot(vector, num_classes):\n",
    "    result = np.zeros(shape=(len(vector), num_classes))\n",
    "    result[np.arange(len(vector)), vector] = 1\n",
    "    return result.astype(int)\n",
    "\n",
    "one_hot_labels = convert_to_one_hot(labels, total_classes)\n",
    "\n",
    "print(\"The original first 5 labels\\n\",labels[:5])\n",
    "print(\"\\nThe first 5 one hot lables\\n\", one_hot_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one last step we should do to properly prepare our data. As of now, we have 150 records we can use for training our neural network. However, if we use all the data for training, we won't have any *clean* data to see how well it performs against new data it hasn't seen before.\n",
    "\n",
    "Fortunately, scikit-learn also gives us an easy function that splits up data into training and testing groups, based on some percentage. We will use X and y as our features and labels for training, and only use X_test and y_test once we've built and trained the model to see how good our model is at making new predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training records: 135\n",
      "\n",
      "total testing records: 15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, X_test, y, y_test = train_test_split(features,one_hot_labels, test_size=0.1)\n",
    "\n",
    "print(\"total training records:\", len(y))\n",
    "print(\"\\ntotal testing records:\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded all our data, properly shuffled the records, one hot encoded the labels and split out a few records to use for final testing, we're ready to build our model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building our Neural Network\n",
    "\n",
    "We are going to build a neural network that takes in four inputs (one per feature) all connected to a hidden layer that has five neurons. This hidden layer is fully connected to a final output layer that has three neurons (one per each class of iris).\n",
    "\n",
    "The secret sauce of a neural network are the *weights* that connect together all these neurons together. For example, each of the four input neurons has a unique weight for each of the 5 neurons it is connected to in the hidden layer. That means there are 4x5 or 20 weights leading into the hidden layer. \n",
    "\n",
    "There is another set of weights for each of the 5 neurons on the hidden layer connecting to each of the 3 output neurons, for a total of 5x3 or 15 weights in the second set.\n",
    "\n",
    "The way our neural network will work is that for each record we use for training, those 4 measurement features are used as our input values **X** for the 4 input neurons. Then, each of those input values are multiplied by their unique weights per neuron **W** in the hidden layer. Now each of the 5 nodes in the hidden layer sums up the four distinct **(X \\* W)** values it received from the input nodes and adds an additional value called a bias. After each neuron has added up all it the values of the inputs neurons multiplied by their unique weights and added a bias factor, the last step a neuron does is make it **non-linear** with an **activation function**. This gives the neuron a bit more wiggle room as it eventually improves and looks for ways to optimize itself through training. While there are multiple types of activation functions, we will use the popular **sigmoid** function as the activation for our hidden layer neurons. A [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) takes any number and squashes it down to a value between 0 and 1.\n",
    "\n",
    "Once all the neurons on the hidden layer have their summed and activated values the received from the input features, this process is repeated again bewteen the hidden layer and the output layer. Each of the 5 hidden layer neurons multiplies its calculated value by its unique weights per output node and sends them along. Each output node sums up all the values it received from the hidden layer. However, the final output layer doesn't use sigmoid, but an activation technique called **softmax** which will take it's three values and convert them into a true probability distribution. This means that like sigmoid, each of the three output neurons (which represent the three possible iris categories) will have their values squashed down to a value bewteen 0 and 1, but softmax takes it a step farther and makes sure that collectively, all three values add up to 1.0 (representing all possible scenarios).\n",
    "\n",
    "Let's first setup these sigmoid and softmax functions, which we will make use of in our model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SIGMOID activation squashes a number onto a curve from 0 to 1\n",
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "# SOFTMAX is used on the output layer to both squash each neuron value\n",
    "# to a number on a curve from 0 to 1 but also make sure that all\n",
    "# of their unique values collectively add up to exactly 1.0, \n",
    "# representing a true probability distribution among all possible categories\n",
    "def softmax(w):\n",
    "    e = np.exp(w - np.amax(w))\n",
    "    dist = e / np.sum(e)\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've discussed so far is called the **forward pass**. The flow is input values (**X**) come in from the features of each record, these input values are multiplied by their unique weights and summed together in each neuron on the hidden layer. The hidden layer adds an additional bias term (treated as an additional weight in practice), and then applies a sigmoid activation to the result. This final activation value is then multipled by it's own unique set of weights and those values are summed together in each of the three ouput layer neurons. The output layer applies softmax onto its three neuron values to produce a final probability distribution that should match the *one hot encoded* label for that record, which is what the answer *should* be.\n",
    "\n",
    "All this sounds well and good. But you need to remember that the important thing here as the programmer is that we build the *model* correctly, not that we figure out the secret sauce, the weights, ourselves. That's the whole point of a neural network. But until we have a good set of weight values, our neural network, even if it is flawlessly built, is still going to be useless and just spew out junk. \n",
    "\n",
    "It is important to realize this early on, and I don't believe this can be stated explicitly enough when you're first starting out - when we first build our neural network, *all these weights are initialized to just random garbage.* That means it doesn't matter how many *forward passes* you run through, nothing is going to change. Those initial random weights, which are rubish, are just going to stay rubish and your neural network will never *learn*. That is where **backpropagation** comes in. Here's how it works.\n",
    "\n",
    "When we run a **forward pass**, we end up with a probability distribution that will look something like [.43  .37 .2]. Each value represents the probability that the features map to either a setosa, versicolor or verginica iris. As we've discussed above, these first *several hundred* predictions are going to most likely be horribly, horribly wrong. But that is why we are **training** the neural network with features that have the correct label available. \n",
    "\n",
    "What backpropgation does is it looks at the result of the forwardpass, the predicted answer, and compares it to what the real answer should be. The difference between what the model produced and what the answer should have been now gives us a metric, an error score, we can exploit with the power of calculus! With that error score and a bit of this *calculus magic* (specifically derivatives, partial derivatives and the chain rule) we can start walking backward through the neural network and have each weight calculate which way it should change itself to move in the 'less wrong' direction. By the way, how *big of a step* it takes in this 'optimized' direction is called the **learning rate**. By running this \"forwardpass / backward pass\" training_cycle many, many times, and each time the weights are nudged in a direction that should be slightly more optimal if they were trying to get their calculated prediction closer to what the answer *should* have been, we will slowly *grow* our neural network into a accurately weighted, classification machine.\n",
    "\n",
    "If you want to learn more about backpropagation and how it works, *once you're done with **this** notebook,* check out this [5 minute video](https://www.youtube.com/watch?v=q555kfIFUCM) by Siraj Raval for a great introduction to the topic. \n",
    "\n",
    "For now, though, if you're ok considering backpropagation as a form of *calculus magic*, we can move on. There is one explicit function we will need to define to help us out, though. This function calcuates the derivative of our sigmoid function we defined and which is used in the hidden layer. We need this for backprop as we're calculating how much each weights needs to get tweaked, based on how much it contributed to the final error and how much it actually did. We are also going to import one more scikit-learn function we will use during backprop to figure out our error score. \n",
    "\n",
    "Here they both are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def sigmoidPrime(y):\n",
    "    return np.multiply(y, (1. - y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all that out of the way, we're ready for the main event. Defining our simple Neural Network class. When we initialize it, it will expect to know how many input features it is expected to process (in our case, 4 measurements). It will also want to know how many neurons we want in our hidden layer (we will start with 5 neurons, though this is a **hyper-parameter**, or number you may want to play around with and see if the model trains better or faster ). Finally, it will want to know how many classes should exist in our output layer's probability distribution.\n",
    "\n",
    "We are also adding a reset_all() function so that we can test and poke around this Neural Network once it's working and reset all the learned weights back to initial, randomized values (which will predict garbage again, until we re-train the network again).\n",
    "\n",
    "We define the forward and backward passes as their own functions, though in practice we won't call these independently. We will instead call the **training_cycle()** function, passing it in a record of features (**X**) and one_hot_labels (**y**) to be used for training. \n",
    "\n",
    "Instead of having a discreet training_cycle() function, most Neural Network samples provide a function called fit() or train() that runs this training_cycle many, many times. I wanted to make it a single forward/backward pass function, though, so we can start off very slowly and send a few records in one at a time to see what is actually happening during a training cycle. Once satisfied, we can speed up our training by calling this training_cycle() in it's own loop.\n",
    "\n",
    "Here is the full class definition:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "\n",
    "        self.learning_rate = 0.01\n",
    "        \n",
    "        # A very easy way of adding a bias to each neuron is to treat the bias\n",
    "        # as one more input value, which will be fully connected to all neurons\n",
    "        # in the hidden layer\n",
    "        self.input_size = input_size + 1\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Let's use our own reset_all() function to initialize \n",
    "        # the weights with small, random values\n",
    "        self.reset_all()\n",
    "        \n",
    "    def reset_all(self):\n",
    "        self.total_training_cycle = 0\n",
    "    \n",
    "        # Setup our activations, or initial values, of our neurons\n",
    "        # We can initialize these all to '1' for now.\n",
    "        self.activations_input = np.ones(self.input_size)\n",
    "        self.activations_hidden = np.ones(self.hidden_size)\n",
    "        self.activations_output = np.ones(self.output_size)\n",
    "        \n",
    "        # Setup our weights. Here, we don't want to initialize them to either 1, zero\n",
    "        # the same number as they each need to start and optimize themselves (grow) in\n",
    "        # their own distinct direction. The best thing to do is to fill them up with\n",
    "        # random numbers that cover a small range around zero, like so:\n",
    "        input_range = 0.2\n",
    "\n",
    "        # The size of our input_layer weights is (input_size, hidden_size) or (4, 5)\n",
    "        self.weights_input_layer  = np.random.normal(loc = 0, scale = input_range, size = (self.input_size, self.hidden_size))\n",
    "        \n",
    "        # The size of our output_layer weights is (hidden_size, output_size) or (5, 3)\n",
    "        self.weights_output_layer = np.random.normal(loc = 0, scale = input_range, size = (self.hidden_size, self.output_size))\n",
    "    \n",
    "    \n",
    "    def feedForward(self, inputs):\n",
    "        \n",
    "        # Don't forget, we added an extra input neuron to serve as our hidden neuron's bias value\n",
    "        # so only fill up the first 4 (not 5) vaules with the 4 input features of the record\n",
    "        # we are processing this learning cycle\n",
    "        self.activations_input[0:self.input_size-1] = inputs\n",
    "        \n",
    "        # Since the weights are a matrix, we dot product to multiply them \n",
    "        # together with their corresponding input values\n",
    "        hidden_sums = np.dot(self.weights_input_layer.T, self.activations_input)\n",
    "        # we then apply our activation function on the sum totals to get our neuron's final value\n",
    "        self.activations_hidden = sigmoid(hidden_sums)\n",
    "\n",
    "        # We repeat the process, only this time with the hidden neurons and their weights\n",
    "        # connecting them to the output neurons\n",
    "        output_sums = np.dot(self.weights_output_layer.T, self.activations_hidden)\n",
    "        # we apply softmax instead of sigmoid on the final output layer to get a true\n",
    "        # probability distribution ( all 3 values will add up to 1.0 )\n",
    "        self.activations_output = softmax(output_sums)\n",
    "\n",
    "        # we return our activations_output, which contains 3 values, \n",
    "        # one probability prediction for each of the 3 classes of irises \n",
    "        return self.activations_output\n",
    "    \n",
    "    \n",
    "    def backPropagate(self, targets):\n",
    "        \n",
    "        # the targets are our one_hot_encoded labels values for the record being processed\n",
    "        output_deltas = -(targets - self.activations_output)\n",
    " \n",
    "        # **********************************************************************************\n",
    "        # BEGIN CALCULUS MAGIC. ONCE YOU GROK THE FULL THING, FEEL FREE TO COME BACK \n",
    "        # HERE AND DIVE IN. BUT DON'T GET DISTRACTED UNTIL YOU FIRST FINISH THE FULL\n",
    "        # PROJECT AND SEE IT WORKING. THEN COME BACK HERE AND JUMP IN IF YOU LIKE!\n",
    "        error = np.dot(self.weights_output_layer, output_deltas)\n",
    "        hidden_deltas = sigmoidPrime(self.activations_hidden) * error\n",
    "        \n",
    "        # update the weights connecting the hidden layer to the output layer\n",
    "        output_gradients = output_deltas * np.reshape(self.activations_hidden, (self.activations_hidden.shape[0], 1))\n",
    "        self.weights_output_layer = self.weights_output_layer - self.learning_rate * output_gradients\n",
    "        \n",
    "        # update the weights connecting the input layer to the hidden layer\n",
    "        hidden_gradients = hidden_deltas * np.reshape(self.activations_input, (self.activations_input.shape[0], 1))\n",
    "        self.weights_input_layer = self.weights_input_layer - self.learning_rate * hidden_gradients\n",
    "        # **********************************************************************************\n",
    "        \n",
    "        # This will give us an overal 'error score' we can use to see how well our Neural Network is performing...\n",
    "        log_error = log_loss(targets, self.activations_output)\n",
    "        return log_error\n",
    "    \n",
    "    def training_cycle(self, X, y):\n",
    "        # A training cycle expects the features (X) and \n",
    "        # one_hot_label (y) from a record in our training data\n",
    "        \n",
    "        # we pass the features as our input data for the forward pass...\n",
    "        self.feedForward(X)\n",
    "        \n",
    "        # then compare the prediction against the true answer (y) in our backward pass...\n",
    "        error = self.backPropagate(y)\n",
    "        \n",
    "        self.total_training_cycle += 1\n",
    "        # let's return some extra information we may want to explore as we watch the Neural Network learn and grow\n",
    "        return self.total_training_cycle, self.activations_output, error\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training our Neural Network\n",
    "\n",
    "Now that we've defined our neural network, let's instantiate it and start passing in records from our Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "total_features = 4 # each record has 4 measurement features\n",
    "hidden_neurons = 5 # We want 5 neurons on our hidden layer\n",
    "output_neurons = 3 # our output layer has one neuron per flower category\n",
    "\n",
    "NN = Neural_Network(total_features, hidden_neurons, output_neurons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the two sets of weights look like. In fact, if you run the following cell multiple times, you will see that each time the weights will be different, since they are always initialized to just random junk and will need to be grow over time through many, many training_cycle() calls..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZED WEIGHTS_INPUT_LAYER:\n",
      "[[ 0.2693492   0.02535863 -0.15192788 -0.00067017 -0.01379613]\n",
      " [-0.51564309 -0.11227337  0.06284075 -0.08484311 -0.12924187]\n",
      " [ 0.18227524 -0.18330916  0.14232453 -0.26654889  0.55215239]\n",
      " [-0.04313711 -0.06465306  0.22861318 -0.08374097  0.29098556]\n",
      " [ 0.07431877 -0.02367958 -0.04500118  0.24151843  0.04609744]] \n",
      "\n",
      "INITIALIZED WEIGHTS_INPUT_LAYER:\n",
      "[[ 0.08646175  0.10820037  0.02658724]\n",
      " [ 0.10397445 -0.02366468 -0.00996173]\n",
      " [-0.22382299  0.05955205 -0.3439082 ]\n",
      " [ 0.05671161 -0.0561232   0.07491702]\n",
      " [-0.20976567 -0.42387147 -0.07017306]]\n"
     ]
    }
   ],
   "source": [
    "# In case you want to run this cell multiple times, you'll see we are always resetting\n",
    "# and randomizing our weights when we first initialize our untrained Neural Network\n",
    "NN.reset_all()\n",
    "\n",
    "# LET'S SEE WHAT THE RANDOM WEIGHTS ARE AFTER BUILDING OUR NETWORK\n",
    "print(\"INITIALIZED WEIGHTS_INPUT_LAYER:\")\n",
    "print(NN.weights_input_layer,\"\\n\")\n",
    "\n",
    "print(\"INITIALIZED WEIGHTS_INPUT_LAYER:\")\n",
    "print(NN.weights_output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop: 1  target:  [0 1 0]  =>  [ 0.34420125  0.31466026  0.34113849]   log_error:  0.665134996522\n"
     ]
    }
   ],
   "source": [
    "# LET'S PASS THE FIRST RECORD IN OUR TRAINING DATASET THROUGH OUR NEW NEURAL NETWORK\n",
    "loop_count, prediction, error = NN.training_cycle(X[0], y[0])\n",
    "print(\"loop:\",loop_count,\" target: \", y[0], \" => \", prediction, \"  log_error: \", error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the very first loop produces horrible results, and why shouldn't it? The weights, the secret sauce or the neural network is totally untrained. Actually, it has been slightly trained, since it has experienced it's training cycle, including the backpropagation() that slightly tweaked it's weights. Let's verify this by looking at the weights again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZED WEIGHTS_INPUT_LAYER:\n",
      "[[ 0.08391677  0.11326768  0.0240649 ]\n",
      " [ 0.10302124 -0.02176675 -0.01090645]\n",
      " [-0.22565606  0.0632019  -0.34572497]\n",
      " [ 0.05587271 -0.05445286  0.07408558]\n",
      " [-0.2128824  -0.41766574 -0.07326206]]\n"
     ]
    }
   ],
   "source": [
    "print(\"INITIALIZED WEIGHTS_INPUT_LAYER:\")\n",
    "print(NN.weights_output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the output weights are now *slightly* different. Backpropagation has used that calculus magic to lightly nudge them in a direction that would have gotten their error score a little smaller. Let's call the training_cycle a few more times and see what happens. It will help if we define a singlePass() function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records:\n",
      " [[ 5.5  2.3  4.   1.3]\n",
      " [ 6.3  2.8  5.1  1.5]\n",
      " [ 5.4  3.4  1.5  0.4]\n",
      " [ 6.   2.9  4.5  1.5]\n",
      " [ 6.9  3.1  5.1  2.3]]\n",
      "\n",
      "First 5 matching labels:\n",
      " [[0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "\n",
      "Running the first 5 records through the Neural Network:\n",
      "\n",
      "loop: 2  target:  [0 1 0]  =>  [ 0.37362072  0.36521779  0.26116149]   log_error:  0.592578851778\n",
      "loop: 3  target:  [0 0 1]  =>  [ 0.37290538  0.36834671  0.25874792]   log_error:  0.759324481499\n",
      "loop: 4  target:  [1 0 0]  =>  [ 0.35627257  0.37793727  0.26579016]   log_error:  0.605244649705\n",
      "loop: 5  target:  [0 1 0]  =>  [ 0.37326426  0.36389688  0.26283887]   log_error:  0.594354608299\n",
      "loop: 6  target:  [0 0 1]  =>  [ 0.37142832  0.36774208  0.2608296 ]   log_error:  0.755550349497\n"
     ]
    }
   ],
   "source": [
    "def singlePass(idx):\n",
    "    loop_count, prediction, error = NN.training_cycle(X[idx], y[idx])\n",
    "    print(\"loop:\",loop_count,\" target: \", y[idx], \" => \", prediction, \"  log_error: \", error)\n",
    "\n",
    "print(\"First 5 records:\\n\", X[:5])\n",
    "print(\"\\nFirst 5 matching labels:\\n\", y[:5])\n",
    "\n",
    "print(\"\\nRunning the first 5 records through the Neural Network:\\n\")\n",
    "singlePass(0)\n",
    "singlePass(1)\n",
    "singlePass(2)\n",
    "singlePass(3)\n",
    "singlePass(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log error doesn't really give us much to go on yet, since it's all over the map. That is because there are only three categories, so even with this very young neural network almost blindly guessing, it's still going to be right around 33% of the time! \n",
    "\n",
    "Let's reset our Neural Network and start to run a lot more training_cycles. When you run through all the training data one time, that is usually called an **epoch**. So let's build a function that will run a full epoch, or all 135 training records (don't forget, we still have 15 records set aside to use for testing once we've grown a strong neural network ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch average error:  0.635613824874\n",
      "loop:  136  target:  [0 1 0]  =>  [ 0.29955778  0.33991338  0.36052883]\n"
     ]
    }
   ],
   "source": [
    "# Remember, this resets the weights so they will no longer match\n",
    "# with the above explorations. We are starting fresh and from square one again...\n",
    "NN.reset_all()\n",
    "\n",
    "def run_full_epoch():\n",
    "    # Let's start averaging the epoch error, which will make it\n",
    "    # much more accurate than the individual error per record\n",
    "    # which doesn't account for random guesses being right a third of the time\n",
    "    epoch_error = 0\n",
    "    total_records = len(X)\n",
    "    \n",
    "    # This will run through each record in our dataset once,\n",
    "    # slightly optimizing the weights and thus training our model\n",
    "    # each time we call training_cycle()\n",
    "    for i in range(total_records):\n",
    "        _, prediction, error = NN.training_cycle(X[i], y[i])\n",
    "        epoch_error += error\n",
    "    \n",
    "    # Let's see what our average error rate is for that Epoch of training.\n",
    "    # This is what we will look at to make sure it is going DOWN.\n",
    "    # That means the error is getting smaller, or our neural network is getting smarter!\n",
    "    epoch_error = epoch_error / float(total_records)\n",
    "    print(\"\\nEpoch average error: \", epoch_error )\n",
    "\n",
    "    # After each Epoch of training, let's pull a random sample\n",
    "    # from our dataset to see how well the predictions match up to the one_hot_label\n",
    "    # they're shooting for...\n",
    "    random_sample = int(np.random.randint(0, len(X)-1))\n",
    "    loop_count, prediction, _ = NN.training_cycle(X[random_sample], y[random_sample])\n",
    "    print(\"loop: \", loop_count, \" target: \", y[random_sample], \" => \", prediction)\n",
    "\n",
    "# Let's run a full epoch, or go through each of the 135 records in our dataset once...\n",
    "run_full_epoch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see our average error is around .63 and our prediction still seems pretty much like a pure guess.\n",
    "\n",
    "This isn't good. But it's a start!\n",
    "\n",
    "Let's run through 10 more epochs and see if our average error starts going down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch average error:  0.630789634342\n",
      "loop:  272  target:  [0 1 0]  =>  [ 0.29650255  0.32728224  0.37621521]\n",
      "\n",
      "Epoch average error:  0.626099344244\n",
      "loop:  408  target:  [1 0 0]  =>  [ 0.31410764  0.33649905  0.34939331]\n",
      "\n",
      "Epoch average error:  0.619721270147\n",
      "loop:  544  target:  [0 0 1]  =>  [ 0.27732252  0.31004189  0.41263559]\n",
      "\n",
      "Epoch average error:  0.610521606591\n",
      "loop:  680  target:  [0 0 1]  =>  [ 0.2546135   0.30617147  0.43921503]\n",
      "\n",
      "Epoch average error:  0.597263664073\n",
      "loop:  816  target:  [1 0 0]  =>  [ 0.36663587  0.33142909  0.30193504]\n",
      "\n",
      "Epoch average error:  0.578927551171\n",
      "loop:  952  target:  [0 1 0]  =>  [ 0.29049441  0.32656956  0.38293602]\n",
      "\n",
      "Epoch average error:  0.556195224188\n",
      "loop:  1088  target:  [1 0 0]  =>  [ 0.45151613  0.31029856  0.23818531]\n",
      "\n",
      "Epoch average error:  0.529385568667\n",
      "loop:  1224  target:  [0 1 0]  =>  [ 0.22703763  0.32694988  0.44601249]\n",
      "\n",
      "Epoch average error:  0.501418015058\n",
      "loop:  1360  target:  [0 1 0]  =>  [ 0.23142739  0.33674207  0.43183054]\n",
      "\n",
      "Epoch average error:  0.473918775388\n",
      "loop:  1496  target:  [0 1 0]  =>  [ 0.21261388  0.34715108  0.44023504]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    run_full_epoch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WOOT!**\n",
    "\n",
    "our average error is starting to go down. our predictions are still rubbish, but they are slowly getting better on average. Let's crank up more revolutions. Say 25 more epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch average error:  0.448567024666\n",
      "loop:  1632  target:  [0 0 1]  =>  [ 0.1245329   0.34442173  0.53104537]\n",
      "\n",
      "Epoch average error:  0.426363477651\n",
      "loop:  1768  target:  [0 1 0]  =>  [ 0.20045424  0.36866163  0.43088412]\n",
      "\n",
      "Epoch average error:  0.407129555377\n",
      "loop:  1904  target:  [0 1 0]  =>  [ 0.23083145  0.37915419  0.39001436]\n",
      "\n",
      "Epoch average error:  0.391014724891\n",
      "loop:  2040  target:  [0 0 1]  =>  [ 0.06537836  0.34608456  0.58853708]\n",
      "\n",
      "Epoch average error:  0.377478142141\n",
      "loop:  2176  target:  [1 0 0]  =>  [ 0.78302919  0.15558821  0.0613826 ]\n",
      "\n",
      "Epoch average error:  0.365613883777\n",
      "loop:  2312  target:  [1 0 0]  =>  [ 0.76758564  0.1671593   0.06525507]\n",
      "\n",
      "Epoch average error:  0.355544579925\n",
      "loop:  2448  target:  [1 0 0]  =>  [ 0.84055955  0.12009904  0.03934141]\n",
      "\n",
      "Epoch average error:  0.346847022065\n",
      "loop:  2584  target:  [1 0 0]  =>  [ 0.84662814  0.11771079  0.03566107]\n",
      "\n",
      "Epoch average error:  0.339084253057\n",
      "loop:  2720  target:  [0 0 1]  =>  [ 0.02878808  0.33588543  0.63532649]\n",
      "\n",
      "Epoch average error:  0.332209273399\n",
      "loop:  2856  target:  [1 0 0]  =>  [ 0.87987541  0.09630398  0.02382061]\n",
      "\n",
      "Epoch average error:  0.325592355877\n",
      "loop:  2992  target:  [0 1 0]  =>  [ 0.09018264  0.424543    0.48527436]\n",
      "\n",
      "Epoch average error:  0.319334244856\n",
      "loop:  3128  target:  [1 0 0]  =>  [ 0.88557755  0.09401497  0.02040748]\n",
      "\n",
      "Epoch average error:  0.313744002787\n",
      "loop:  3264  target:  [0 1 0]  =>  [ 0.12120187  0.46184849  0.41694964]\n",
      "\n",
      "Epoch average error:  0.307997118729\n",
      "loop:  3400  target:  [0 1 0]  =>  [ 0.10336336  0.46646067  0.43017596]\n",
      "\n",
      "Epoch average error:  0.302509897183\n",
      "loop:  3536  target:  [0 1 0]  =>  [ 0.0612432   0.44310258  0.49565422]\n",
      "\n",
      "Epoch average error:  0.297138636562\n",
      "loop:  3672  target:  [0 0 1]  =>  [ 0.01603741  0.3265767   0.65738588]\n",
      "\n",
      "Epoch average error:  0.292330386407\n",
      "loop:  3808  target:  [0 0 1]  =>  [ 0.01388015  0.3142769   0.67184295]\n",
      "\n",
      "Epoch average error:  0.287065810257\n",
      "loop:  3944  target:  [1 0 0]  =>  [ 0.8967209   0.09091725  0.01236185]\n",
      "\n",
      "Epoch average error:  0.281660928151\n",
      "loop:  4080  target:  [0 0 1]  =>  [ 0.01854491  0.34604617  0.63540892]\n",
      "\n",
      "Epoch average error:  0.276811957847\n",
      "loop:  4216  target:  [0 0 1]  =>  [ 0.02255543  0.36729229  0.61015228]\n",
      "\n",
      "Epoch average error:  0.271712597718\n",
      "loop:  4352  target:  [0 1 0]  =>  [ 0.04791601  0.46133074  0.49075324]\n",
      "\n",
      "Epoch average error:  0.265753313292\n",
      "loop:  4488  target:  [0 0 1]  =>  [ 0.02149965  0.3756601   0.60284025]\n",
      "\n",
      "Epoch average error:  0.261394212446\n",
      "loop:  4624  target:  [0 0 1]  =>  [ 0.00972367  0.28921407  0.70106227]\n",
      "\n",
      "Epoch average error:  0.255987515438\n",
      "loop:  4760  target:  [1 0 0]  =>  [ 0.91345439  0.08022816  0.00631745]\n",
      "\n",
      "Epoch average error:  0.250633598035\n",
      "loop:  4896  target:  [0 0 1]  =>  [ 0.00678772  0.24786446  0.74534782]\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    run_full_epoch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice.\n",
    "\n",
    "Our average error is still going down and now we can see that our sample predictions are returning accurate results for the most part. Let's hit it with another 30 epochs of training and see how low we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch average error:  0.245658417988\n",
      "loop:  5032  target:  [0 0 1]  =>  [ 0.01608487  0.34978043  0.6341347 ]\n",
      "\n",
      "Epoch average error:  0.240878907391\n",
      "loop:  5168  target:  [0 0 1]  =>  [ 0.00802199  0.26666623  0.72531178]\n",
      "\n",
      "Epoch average error:  0.235623977402\n",
      "loop:  5304  target:  [0 1 0]  =>  [ 0.21382681  0.6131437   0.17302948]\n",
      "\n",
      "Epoch average error:  0.230378797318\n",
      "loop:  5440  target:  [0 0 1]  =>  [ 0.01486166  0.35672817  0.62841017]\n",
      "\n",
      "Epoch average error:  0.225946741653\n",
      "loop:  5576  target:  [1 0 0]  =>  [ 0.90223454  0.09300606  0.0047594 ]\n",
      "\n",
      "Epoch average error:  0.220638425339\n",
      "loop:  5712  target:  [1 0 0]  =>  [ 0.92016144  0.07656778  0.00327077]\n",
      "\n",
      "Epoch average error:  0.215914576195\n",
      "loop:  5848  target:  [0 0 1]  =>  [ 0.00398369  0.19417624  0.80184007]\n",
      "\n",
      "Epoch average error:  0.211361469717\n",
      "loop:  5984  target:  [0 1 0]  =>  [ 0.07432277  0.63040103  0.2952762 ]\n",
      "\n",
      "Epoch average error:  0.206368959141\n",
      "loop:  6120  target:  [1 0 0]  =>  [ 0.91849981  0.07881315  0.00268704]\n",
      "\n",
      "Epoch average error:  0.202190912402\n",
      "loop:  6256  target:  [0 1 0]  =>  [ 0.04121259  0.56011147  0.39867593]\n",
      "\n",
      "Epoch average error:  0.197444756764\n",
      "loop:  6392  target:  [1 0 0]  =>  [ 0.92974167  0.06836843  0.0018899 ]\n",
      "\n",
      "Epoch average error:  0.193594921997\n",
      "loop:  6528  target:  [1 0 0]  =>  [ 0.92796226  0.07018213  0.0018556 ]\n",
      "\n",
      "Epoch average error:  0.189482437435\n",
      "loop:  6664  target:  [0 0 1]  =>  [ 0.00275131  0.16231471  0.83493397]\n",
      "\n",
      "Epoch average error:  0.185572322175\n",
      "loop:  6800  target:  [1 0 0]  =>  [ 0.9142407   0.08363827  0.00212103]\n",
      "\n",
      "Epoch average error:  0.181569769005\n",
      "loop:  6936  target:  [1 0 0]  =>  [ 0.9363412   0.0623628   0.00129601]\n",
      "\n",
      "Epoch average error:  0.17780614222\n",
      "loop:  7072  target:  [0 1 0]  =>  [ 0.04668895  0.62902641  0.32428464]\n",
      "\n",
      "Epoch average error:  0.17379162036\n",
      "loop:  7208  target:  [0 0 1]  =>  [ 0.00867717  0.32673224  0.66459059]\n",
      "\n",
      "Epoch average error:  0.171042611792\n",
      "loop:  7344  target:  [0 1 0]  =>  [ 0.03135294  0.59247099  0.37617607]\n",
      "\n",
      "Epoch average error:  0.16676470708\n",
      "loop:  7480  target:  [0 0 1]  =>  [ 0.003579    0.20271925  0.79370175]\n",
      "\n",
      "Epoch average error:  0.163991497909\n",
      "loop:  7616  target:  [0 1 0]  =>  [ 0.0760573   0.76611148  0.15783122]\n",
      "\n",
      "Epoch average error:  0.160420274251\n",
      "loop:  7752  target:  [1 0 0]  =>  [  9.40883415e-01   5.82715761e-02   8.45008535e-04]\n",
      "\n",
      "Epoch average error:  0.157453877914\n",
      "loop:  7888  target:  [0 1 0]  =>  [ 0.06284994  0.7654815   0.17166856]\n",
      "\n",
      "Epoch average error:  0.154292811496\n",
      "loop:  8024  target:  [0 1 0]  =>  [ 0.05696061  0.7784287   0.16461069]\n",
      "\n",
      "Epoch average error:  0.151398617734\n",
      "loop:  8160  target:  [0 1 0]  =>  [ 0.00670415  0.32449785  0.66879799]\n",
      "\n",
      "Epoch average error:  0.148538843045\n",
      "loop:  8296  target:  [0 0 1]  =>  [ 0.00663173  0.31534943  0.67801883]\n",
      "\n",
      "Epoch average error:  0.146563311101\n",
      "loop:  8432  target:  [1 0 0]  =>  [  9.50452854e-01   4.90277735e-02   5.19372717e-04]\n",
      "\n",
      "Epoch average error:  0.143524259144\n",
      "loop:  8568  target:  [0 0 1]  =>  [ 0.00240114  0.17161191  0.82598695]\n",
      "\n",
      "Epoch average error:  0.141177611993\n",
      "loop:  8704  target:  [0 1 0]  =>  [ 0.01580105  0.50030882  0.48389012]\n",
      "\n",
      "Epoch average error:  0.138379622891\n",
      "loop:  8840  target:  [0 1 0]  =>  [ 0.023206    0.62653882  0.35025518]\n",
      "\n",
      "Epoch average error:  0.136048877173\n",
      "loop:  8976  target:  [0 1 0]  =>  [ 0.0477521   0.78282628  0.16942162]\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    run_full_epoch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch average error:  0.115360887617\n",
      "loop:  10472  target:  [0 0 1]  =>  [  4.58138259e-04   6.46581629e-02   9.34883699e-01]\n",
      "\n",
      "Epoch average error:  0.113859561054\n",
      "loop:  10608  target:  [0 1 0]  =>  [ 0.024821    0.75625344  0.21892556]\n",
      "\n",
      "Epoch average error:  0.112267572556\n",
      "loop:  10744  target:  [0 0 1]  =>  [ 0.00423611  0.2994531   0.69631079]\n",
      "\n",
      "Epoch average error:  0.111284499649\n",
      "loop:  10880  target:  [1 0 0]  =>  [  9.59954140e-01   3.98353141e-02   2.10545784e-04]\n",
      "\n",
      "Epoch average error:  0.109537652705\n",
      "loop:  11016  target:  [0 0 1]  =>  [  3.75934118e-04   5.60469590e-02   9.43577107e-01]\n",
      "\n",
      "Epoch average error:  0.10822115072\n",
      "loop:  11152  target:  [0 1 0]  =>  [ 0.04601588  0.88427512  0.06970899]\n",
      "\n",
      "Epoch average error:  0.106877787148\n",
      "loop:  11288  target:  [0 0 1]  =>  [  5.40714140e-04   7.59178147e-02   9.23541471e-01]\n",
      "\n",
      "Epoch average error:  0.105679360796\n",
      "loop:  11424  target:  [0 0 1]  =>  [ 0.00219782  0.21704295  0.78075923]\n",
      "\n",
      "Epoch average error:  0.104663608881\n",
      "loop:  11560  target:  [0 0 1]  =>  [  3.62336767e-04   5.58410504e-02   9.43796613e-01]\n",
      "\n",
      "Epoch average error:  0.10327695489\n",
      "loop:  11696  target:  [1 0 0]  =>  [  9.54352031e-01   4.54369883e-02   2.10980818e-04]\n",
      "\n",
      "Epoch average error:  0.102128413455\n",
      "loop:  11832  target:  [0 1 0]  =>  [ 0.00866578  0.48406822  0.507266  ]\n",
      "\n",
      "Epoch average error:  0.10133166317\n",
      "loop:  11968  target:  [0 0 1]  =>  [ 0.00138195  0.1548519   0.84376615]\n",
      "\n",
      "Epoch average error:  0.100058184691\n",
      "loop:  12104  target:  [0 0 1]  =>  [ 0.00104914  0.12343982  0.87551104]\n",
      "\n",
      "Epoch average error:  0.0989721557387\n",
      "loop:  12240  target:  [1 0 0]  =>  [  9.63430291e-01   3.64222032e-02   1.47505360e-04]\n",
      "\n",
      "Epoch average error:  0.0979105725946\n",
      "loop:  12376  target:  [0 0 1]  =>  [  3.99838136e-04   6.47412191e-02   9.34858943e-01]\n",
      "\n",
      "Epoch average error:  0.0969511401388\n",
      "loop:  12512  target:  [1 0 0]  =>  [  9.57892515e-01   4.19425226e-02   1.64962539e-04]\n",
      "\n",
      "Epoch average error:  0.0959898902145\n",
      "loop:  12648  target:  [1 0 0]  =>  [  9.57277825e-01   4.25589281e-02   1.63246400e-04]\n",
      "\n",
      "Epoch average error:  0.0950718152218\n",
      "loop:  12784  target:  [0 1 0]  =>  [ 0.03951626  0.91394299  0.04654075]\n",
      "\n",
      "Epoch average error:  0.0941667304692\n",
      "loop:  12920  target:  [0 0 1]  =>  [  1.42446832e-04   3.12473393e-02   9.68610214e-01]\n",
      "\n",
      "Epoch average error:  0.093311544571\n",
      "loop:  13056  target:  [0 0 1]  =>  [ 0.00272604  0.26549686  0.7317771 ]\n",
      "\n",
      "Epoch average error:  0.0927115127305\n",
      "loop:  13192  target:  [1 0 0]  =>  [  9.61489571e-01   3.83783028e-02   1.32125772e-04]\n",
      "\n",
      "Epoch average error:  0.0916406342984\n",
      "loop:  13328  target:  [0 0 1]  =>  [  2.08084846e-04   4.06059278e-02   9.59185987e-01]\n",
      "\n",
      "Epoch average error:  0.0908504663259\n",
      "loop:  13464  target:  [0 0 1]  =>  [  4.27928553e-04   6.93732055e-02   9.30198866e-01]\n",
      "\n",
      "Epoch average error:  0.0900832128279\n",
      "loop:  13600  target:  [1 0 0]  =>  [  9.62899368e-01   3.69817856e-02   1.18845987e-04]\n",
      "\n",
      "Epoch average error:  0.0893185458244\n",
      "loop:  13736  target:  [1 0 0]  =>  [  9.71228977e-01   2.86840344e-02   8.69882787e-05]\n",
      "\n",
      "Epoch average error:  0.0885870115635\n",
      "loop:  13872  target:  [1 0 0]  =>  [  9.65658150e-01   3.42353924e-02   1.06457441e-04]\n",
      "\n",
      "Epoch average error:  0.0878749941755\n",
      "loop:  14008  target:  [1 0 0]  =>  [  9.71379163e-01   2.85372412e-02   8.35953376e-05]\n",
      "\n",
      "Epoch average error:  0.0871823850409\n",
      "loop:  14144  target:  [0 1 0]  =>  [ 0.03494821  0.93053594  0.03451585]\n",
      "\n",
      "Epoch average error:  0.0865031192716\n",
      "loop:  14280  target:  [0 1 0]  =>  [ 0.09044892  0.90307072  0.00648036]\n",
      "\n",
      "Epoch average error:  0.0858468060737\n",
      "loop:  14416  target:  [0 1 0]  =>  [ 0.02154928  0.90854343  0.06990729]\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    run_full_epoch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After almost 100 Epochs of training, with each Epoch giving the Neural Network exposure to the full dataset, we have an average error rate under 10%! That's great. We could keep going, though we would eventually hit a plateau most likely and never get it all the way down to zero. \n",
    "\n",
    "## Step 4: Testing our Neural Network\n",
    "\n",
    "For now, though, this is more than enough to prove that our Neural Network works, learns by being exposed to data through the forward and backward passes, and has grown a nice collection of weights. Let's use our testing data now and see how our Neural Network stands up to new data it has never seen before.\n",
    "\n",
    "For testing, we won't call a full training cycle anymore but just call the forward pass directly and see what our trained weights get us. We split out 15 records for testing from our original Iris dataset, so let's just run through them all.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0 \n",
      "Target: [0 1 0] \n",
      "Prediction:  [ 0.0084245   0.64473482  0.34684068]\n",
      "Test: 1 \n",
      "Target: [1 0 0] \n",
      "Prediction:  [  9.62175655e-01   3.77177240e-02   1.06621302e-04]\n",
      "Test: 2 \n",
      "Target: [0 1 0] \n",
      "Prediction:  [ 0.0459506   0.93628104  0.01776837]\n",
      "Test: 3 \n",
      "Target: [0 0 1] \n",
      "Prediction:  [  5.94041677e-04   9.75696254e-02   9.01836333e-01]\n",
      "Test: 4 \n",
      "Target: [0 0 1] \n",
      "Prediction:  [  2.25100103e-04   4.75673453e-02   9.52207555e-01]\n",
      "Test: 5 \n",
      "Target: [1 0 0] \n",
      "Prediction:  [  9.69292761e-01   3.06223645e-02   8.48745599e-05]\n",
      "Test: 6 \n",
      "Target: [1 0 0] \n",
      "Prediction:  [  9.66004420e-01   3.39020587e-02   9.35214425e-05]\n",
      "Test: 7 \n",
      "Target: [0 0 1] \n",
      "Prediction:  [ 0.00093543  0.13727543  0.86178914]\n",
      "Test: 8 \n",
      "Target: [0 1 0] \n",
      "Prediction:  [ 0.04571727  0.93677001  0.01751273]\n",
      "Test: 9 \n",
      "Target: [0 1 0] \n",
      "Prediction:  [ 0.12464612  0.86967803  0.00567584]\n",
      "Test: 10 \n",
      "Target: [0 0 1] \n",
      "Prediction:  [  3.69233349e-04   6.49375410e-02   9.34693226e-01]\n",
      "Test: 11 \n",
      "Target: [1 0 0] \n",
      "Prediction:  [  9.69730403e-01   3.01858229e-02   8.37743331e-05]\n",
      "Test: 12 \n",
      "Target: [1 0 0] \n",
      "Prediction:  [  9.65251777e-01   3.46508816e-02   9.73417255e-05]\n",
      "Test: 13 \n",
      "Target: [0 1 0] \n",
      "Prediction:  [ 0.04055065  0.94083429  0.01861506]\n",
      "Test: 14 \n",
      "Target: [1 0 0] \n",
      "Prediction:  [  9.65285072e-01   3.46183307e-02   9.65972913e-05]\n"
     ]
    }
   ],
   "source": [
    "total_test_records = len(y_test)\n",
    "\n",
    "for i in range(total_test_records):\n",
    "    test_result = NN.feedForward(X_test[i])\n",
    "    print(\"Test:\", i, \"\\nTarget:\", y_test[i],\"\\nPrediction: \", test_result)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As a quick side note, that **e-01** or **e-02** etc. at the end of some results is scientific notation. In this context, it means you move the decimal to the left that many places, since the **e** number in these results is negative (-01, -02, ...). When the e number is positive, btw, it means you move the decimal to the *right* that many places, but none of our results here would have a positive e number, since we're dealing with partial percentages that add up to 1.0. \n",
    "\n",
    "So, for example, a value of **9.65e-01** means you move the decimal to the left, resulting in a standard notation of **.965**. If this is new to you, here is a short cut: since the farther to the left the decimal is, the smaller the prediction value is, just look for the smallest **e-** negative number to let you know which of the three numbers is largest. A number with **e-02** at the end of it will be larger than a number with **e-03**  \n",
    "\n",
    "How did you do?!\n",
    "\n",
    "All 15 of my test records were successfully predicted by our now trained and properly weighted neural net. That's pretty amazing when you think about it. \n",
    "\n",
    "## IN CONCLUSION\n",
    "\n",
    "We, as programmers, didn't *solve the problem* of figuring out how to properly convert four iris measurements to figure out how to properly classify them. That is what **classical computer science** does. As programmers, we ourselves are the ones that create the algorithm, or secret sauce that solves the problem.\n",
    "\n",
    "Instead, with **multilayer neural nets**, what we do is build a brain, an artificial neural network and make sure the *structure* is correct: that we can feed it the features, that we have created enough neurons, each with weights and and activation function, and then measured the results. As long as we have enough data, and our brain can feedforward that data to produce a prediction, measure *how wrong* that prediction is against what was expected, then use backpropagation and some calculus magic to slightly nudge it's weights in a more optimium direction, our artificial brain will **itself** learn to solve the problem.\n",
    "\n",
    "This is the biggest mental shift to grasp when switching from classical computer programming to AI programming and deep learning. We no longer create the secret sauce ourselves. We build a brain that can take in large amounts of data and learn for itself what the secret sauce is. \n",
    "\n",
    "That is why this AI boom is so important. For decades, we as humans were creating software using classical programming, but the assumption has always been we can only write solutions for problems we as humans are smart enough to solve. But there is an entire universe of problems that are too complex for us to solve with our human brains.\n",
    "\n",
    "AI starts where classical programming meets its limit. If we *can* solve a program using classical programming, then do it. If you do it right, the output should be 100% correct all the time. But for all those problems we can't solve with 100% certainty, as long as we have enough real world data of previous inputs that lead to various outputs, we can now build an **artificial brain** that can chew over that data and see patterns we can't. Give it enough data, and it can learn the data well enough to be able to make predictions on new information. That's amazing. And that's A.I.\n",
    "\n",
    "@rickbarraza\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
